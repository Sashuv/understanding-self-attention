{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEMk9G6zUo91",
        "outputId": "e42a9e7d-dbe3-47e1-bef6-c7e10cb109de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79b3ce5086f0>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = 4\n",
        "batch = 2\n",
        "no_of_tokens = 5\n",
        "# hyperparameter\n",
        "d_out = 3\n",
        "inputToken = torch.rand(no_of_tokens,d_in)\n",
        "batch_data = torch.stack((inputToken, inputToken))\n",
        "print(batch_data)\n",
        "print(\"Shape:\", batch_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JocddbBUgW6",
        "outputId": "4c9a06cd-5783-46f7-d35c-f961f2af7786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.2961, 0.5166, 0.2517, 0.6886],\n",
            "         [0.0740, 0.8665, 0.1366, 0.1025],\n",
            "         [0.1841, 0.7264, 0.3153, 0.6871],\n",
            "         [0.0756, 0.1966, 0.3164, 0.4017],\n",
            "         [0.1186, 0.8274, 0.3821, 0.6605]],\n",
            "\n",
            "        [[0.2961, 0.5166, 0.2517, 0.6886],\n",
            "         [0.0740, 0.8665, 0.1366, 0.1025],\n",
            "         [0.1841, 0.7264, 0.3153, 0.6871],\n",
            "         [0.0756, 0.1966, 0.3164, 0.4017],\n",
            "         [0.1186, 0.8274, 0.3821, 0.6605]]])\n",
            "Shape: torch.Size([2, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4pSKD0OK2Jg"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, context_len, no_of_tokens, d_in, d_out, dropout):\n",
        "        super(MaskedSelfAttention, self).__init__()\n",
        "        self.QueryM = nn.Linear(d_in, d_out)\n",
        "        self.KeyM   = nn.Linear(d_in, d_out)\n",
        "        self.ValueM = nn.Linear(d_in, d_out)\n",
        "        self.mask = torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputToken):\n",
        "        B, T, C = inputToken.size()  # Batch, Time, Embedding dim\n",
        "        queries = self.QueryM(inputToken)\n",
        "        keys = self.KeyM(inputToken)\n",
        "        values = self.ValueM(inputToken)\n",
        "\n",
        "        attention_scores = queries @ keys.transpose(1, 2)  # shape: [B, T, T]\n",
        "\n",
        "        # Make sure mask is on the same device and dtype\n",
        "        mask = self.mask[:T, :T].to(attention_scores.device).bool()\n",
        "        attention_scores = attention_scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
        "        attention_weights = torch.softmax(attention_scores / (C ** 0.5), dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        context_vector = attention_weights @ values\n",
        "\n",
        "        return context_vector, queries, keys, values, attention_weights, attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AttentionObj = MaskedSelfAttention(6,5,4,3,0)\n",
        "AttentionObj.mask.bool()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrNukkt_LAU0",
        "outputId": "3ab27a8f-ec46-42ea-83a2-25728ef5e069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True,  True],\n",
              "        [False, False, False,  True,  True,  True],\n",
              "        [False, False, False, False,  True,  True],\n",
              "        [False, False, False, False, False,  True],\n",
              "        [False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector, queries, keys, values, attention_weights, maskedAttention = AttentionObj(batch_data)\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nxa_0iGF3S3",
        "outputId": "4fc7cd81-3110-44ae-f959-637ec527a4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2011, -0.3797, -0.0952],\n",
            "         [-0.2657, -0.5626, -0.2966],\n",
            "         [-0.2420, -0.5398, -0.2594],\n",
            "         [-0.2626, -0.4917, -0.2129],\n",
            "         [-0.2497, -0.5053, -0.2158]],\n",
            "\n",
            "        [[-0.2011, -0.3797, -0.0952],\n",
            "         [-0.2657, -0.5626, -0.2966],\n",
            "         [-0.2420, -0.5398, -0.2594],\n",
            "         [-0.2626, -0.4917, -0.2129],\n",
            "         [-0.2497, -0.5053, -0.2158]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}