# 🔍 Self-Attention from Scratch

This repo contains clear, step-by-step implementations of how attention works inside transformer models — starting from the basics and building up to more advanced forms like **masked** and **multi-headed attention**.

---

## 📘 What's Inside

| Notebook | Description |
|----------|-------------|
| `Self_Attention_Implementation.ipynb` | Implements manual self-attention for a single token and then all tokens. Focuses on understanding Query, Key, and Value projections. |
| `Masked_Self_attention.ipynb` | Adds masking to prevent future token access, as used in autoregressive models like GPT. |
| `multi_head_attention.ipynb` | [Coming soon] Implements multi-headed attention to capture diverse feature subspaces across different heads. |

---


> “Attention is all you need — Research Paper”

