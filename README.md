# ğŸ” Self-Attention from Scratch

This repo contains clear, step-by-step implementations of how attention works inside transformer models â€” starting from the basics and building up to more advanced forms like **masked** and **multi-headed attention**.

---

## ğŸ“˜ What's Inside

| Notebook | Description |
|----------|-------------|
| `Self_Attention_Implementation.ipynb` | Implements manual self-attention for a single token and then all tokens. Focuses on understanding Query, Key, and Value projections. |
| `Masked_Self_attention.ipynb` | Adds masking to prevent future token access, as used in autoregressive models like GPT. |
| `multi_head_attention.ipynb` | [Coming soon] Implements multi-headed attention to capture diverse feature subspaces across different heads. |

---


> â€œAttention is all you need â€” Research Paperâ€

